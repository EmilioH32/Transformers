{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from jax import jit\n",
    "import trio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ec397",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenization\n",
    "\n",
    "Based off of Andrej Kapathy YouTube Video \"Let's Build GPT Tokenizer\"\n",
    "\n",
    "``` Place holder for BPE Description```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8168a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test string. It has no meaning and serves for testing purposes only. I'll also add the common sentence with every letter next. The quick brown fox jumps over the lazy dog.\"\n",
    "encoded_input = text.encode(\"utf-8\")\n",
    "encoded_input = list(map(int, encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c831a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(tokens):\n",
    "    counts = {}\n",
    "\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c183db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "pairs = find_pairs(encoded_input)\n",
    "\n",
    "most_freq_pair = max(pairs, key = pairs.get)\n",
    "\n",
    "def merge_most_frequent(id_list, pair, new_token):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(id_list):\n",
    "        if i < len(id_list) - 1 and id_list[i] == pair[0] and id_list[i+1] == pair[1]:\n",
    "            tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            tokens.append(id_list[i])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "# test\n",
    "print(merge_most_frequent([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c430e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (115, 32) into a new token 256\n",
      "merging (115, 32) into a new token 257\n",
      "merging (115, 32) into a new token 258\n",
      "merging (115, 32) into a new token 259\n",
      "merging (115, 32) into a new token 260\n",
      "merging (115, 32) into a new token 261\n",
      "merging (115, 32) into a new token 262\n",
      "merging (115, 32) into a new token 263\n",
      "merging (115, 32) into a new token 264\n",
      "merging (115, 32) into a new token 265\n",
      "merging (115, 32) into a new token 266\n",
      "merging (115, 32) into a new token 267\n",
      "merging (115, 32) into a new token 268\n",
      "merging (115, 32) into a new token 269\n",
      "merging (115, 32) into a new token 270\n",
      "merging (115, 32) into a new token 271\n",
      "merging (115, 32) into a new token 272\n",
      "merging (115, 32) into a new token 273\n",
      "merging (115, 32) into a new token 274\n",
      "merging (115, 32) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "id_list = list(encoded_input) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = find_pairs(id_list)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  new_token = 256 + i\n",
    "  print(f\"merging {pair} into a new token {new_token}\")\n",
    "  ids = merge_most_frequent(id_list, pair, new_token)\n",
    "  merges[pair] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b297240",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
