{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from jax import jit\n",
    "import trio\n",
    "import tiktoken\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ec397",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenization\n",
    "\n",
    "Based off of Andrej Kapathy YouTube Video \"Let's Build GPT Tokenizer\"\n",
    "\n",
    "``` Place holder for BPE Description```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8168a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test string. It has no meaning and serves for testing purposes only. I'll also add the common sentence with every letter next. The quick brown fox jumps over the lazy dog.\"\n",
    "encoded_input = text.encode(\"utf-8\")\n",
    "encoded_input = list(map(int, encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c831a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(tokens):\n",
    "    counts = {}\n",
    "\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = find_pairs(encoded_input)\n",
    "\n",
    "most_freq_pair = max(pairs, key = pairs.get)\n",
    "\n",
    "def merge_most_frequent(id_list, pair, new_token):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(id_list):\n",
    "        if i < len(id_list) - 1 and id_list[i] == pair[0] and id_list[i+1] == pair[1]:\n",
    "            tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            tokens.append(id_list[i])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "# test\n",
    "print(merge_most_frequent([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "id_list = list(encoded_input) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = find_pairs(id_list)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  new_token = 256 + i\n",
    "  print(f\"merging {pair} into a new token {new_token}\")\n",
    "  ids = merge_most_frequent(id_list, pair, new_token)\n",
    "  merges[pair] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b297240",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = find_pairs(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge_most_frequent(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dfaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43372bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f43822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
